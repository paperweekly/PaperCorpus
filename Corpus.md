# Ubuntu Dialogue Corpus
## Source
[The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems](http://aclweb.org/anthology/W/W15/W15-4640.pdf)
## Link
[Dataset Link](https://github.com/rkadlec/ubuntu-ranking-dataset-creator)
## Description
a dataset containing almost 1 million multi-turn dialogues, with a total of over 7 million utterances and 100 million words. This provides a unique resource for research into building dialogue managers based on neural language models that can make use of large amounts of unlabeled data. 
## Time
2015
## Tag
Multi-Turn Dialogue System
## Organization
(1)School of Computer Science, McGill University, Montreal, Canada
(2)Dept Computer Science and Operations Research, Université de Montréal, Montreal, Canada
## Language
English
## Target
(1)[Improved Deep Learning Baselines for Ubuntu Corpus Dialogs](http://arxiv.org/pdf/1510.03753v2.pdf)

# LAMBADA Dataset
## Source
[The LAMBADA dataset:Word prediction requiring a broad discourse context](http://aclweb.org/anthology/P/P16/P16-1144.pdf)
## Link 
[Dataset Link](http://clic.cimec.unitn.it/lambada/)
## Description
A dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word. To succeed on LAMBADA,computational models cannot simply rely on local context, but must be able to keep track of information in the broader discourse.
## Time
2016
## Tag
Word Prediction Task
## Language
English
## Organization
(1)CIMeC - Center for Mind/Brain Sciences, University of Trento
(2)Institute for Logic, Language & Computation, University of Amsterdam
## Target

# CNN/Daily Mail Dataset
## Source
[Teaching Machines to Read and Comprehend](http://cn.arxiv.org/pdf/1506.03340.pdf)
## Link
[Dataset Link](https://github.com/deepmind/rc-data)
## Description
Inspired by work in summarization, we create two machine reading corpora by exploiting online newspaper articles and their matching summaries. We have collected 93k articles from the CNN and 220k articles from the Daily Mail websites. Both news providers supplement their articles with a number of bullet points, summarizing aspects of the information contained in the article. Of key importance is that these summary points are abstractive and do not simply copy sentences from the documents. We construct a corpus of document–query–answer triples by turning these bullet points into Cloze style questions by replacing one entity at a time with a placeholder.
## Time
2015
## Tag
Machine Reading Comprehension
## Organization
(1)Google DeepMind 
(2)University of Oxford
## Language
English
## Target

# Children’s Book Test Dataset
## Source
[THE GOLDILOCKS PRINCIPLE: READING CHILDREN’S BOOKS WITH EXPLICIT MEMORY REPRESENTATIONS](http://cn.arxiv.org/pdf/1511.02301.pdf)
## Link
[Dataset Link](http://www.thespermwhale.com/jaseweston/babi/CBTest.tgz)
## Description
The CBT is built from books that are freely available thanks to Project Gutenberg. Using children’s books guarantees a clear narrative structure, which can make the role of context more salient.
In each question, the first 20 sentences form the context (denoted S), and a word (denoted a) is removed from the 21st sentence, which becomes the query (denoted q). Models must identify the answer word a among a selection of 10 candidate answers (denoted C) appearing in the context sentences and the query.
## Time
2016
## Tag
Machine Reading Comprehension
## Organization
Facebook AI Research
## Language
English
## Target


# SQuAD Dataset
## Source
[SQuAD: 100,000+ Questions for Machine Comprehension of Text](http://cn.arxiv.org/pdf/1606.05250v1)
## Link
[Dataset Link](https://rajpurkar.github.io/SQuAD-explorer/)
## Description
SQuAD, consisting of 100,000+ questions posed by crowdworkers on a set of
Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage.
## Time
2016
## Tag
Machine Reading Comprehension 
## Organization
Computer Science Department, Stanford University
## Language
English
## Target
(1) [MACHINE COMPREHENSION USING MATCH-LSTM AND ANSWER POINTER](http://cn.arxiv.org/pdf/1608.07905v1.pdf)

# People Daily and Children’s Fairy Tale Datasets
## Source
[Consensus Attention-based Neural Networks for Chinese Reading Comprehension](https://arxiv.org/pdf/1607.02250v2.pdf)
## Link
[Dataset Link](http://hfl.iflytek.com/chinese-rc/)
## Description
Though many solid works on previously described public datasets, there is no studies on Chinese reading comprehension datasets.
(1) As far as we know, this is the first Chinese Cloze-style reading comprehension datasets.
(2) We provide a large-scale Chinese reading comprehension data in news domain, as well as its validation and test data as the in-domain test.
(3) Further, we release two out-of-domain test sets, and it deserves to highlight that one of the test sets is made by the humans, which makes it harder to answer than the automatically generated test set.
## Time
2016
## Tag
Machine Reading Comprehension 
## Organization
(1) iFLYTEK Research, China
(2) Research Center for Social Computing and Information Retrieval,
Harbin Institute of Technology, China
## Language 
Chinese
## Target
(1) [Attention-over-Attention Neural Networks for Reading Comprehension](https://arxiv.org/pdf/1607.04423v3.pdf)

# Factoid Question-Answer Corpus
## Source
[Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus](https://arxiv.org/pdf/1603.06807v2.pdf)
## Link
[Dataset Link](http://agarciaduran.org/)
## Description
We present the 30M Factoid QuestionAnswer Corpus, an enormous question answer pair corpus produced by applying a novel neural network architecture on the knowledge base Freebase to transduce facts into natural language questions. The produced question answer pairs are evaluated both by human evaluators and using automatic evaluation metrics, including well-established machine translation and sentence similarity metrics.
## Time
2016
## Tag
Question Answer 
## Organization
(1) University of Montreal, Canada
(2) Universite de Technologie de Compiegne - CNRS, France
## Language 
English
## Target

# LCSTS Dataset
## Source
[LCSTS: A Large-Scale Chinese Short Text Summarization Dataset](http://arxiv.org/pdf/1506.05865v4.pdf)
## Link
[Dataset Link](http://icrc.hitsz.edu.cn/Article/show/139.html)
## Description
We introduce a Large-scale Chinese Short Text Summarization dataset constructed from the Chinese microblogging website SinaWeibo. This corpus consists of over 2 million real Chinese short texts with short summaries given by the writer of each text. We also manually tagged the relevance of 10,666 short summaries with their corresponding short texts. 
## Time
2015
## Tag
Text Summarization
## Organization
(1) Intelligent Computing Research Center, Harbin Institute of Technology, Shenzhen Graduate School
## Language 
Chinese
## Target

# ROCStories Dataset
## Source
[A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories](http://arxiv.org/pdf/1604.01696v1.pdf)
## Link
[Dataset Link](http://cs.rochester.edu/nlp/rocstories/)
## Description
'Story Cloze Test' is a new framework for evaluating story understanding, story generation, and script learning. This test requires a system to choose the correct ending to a four-sentence story. We propose the Story Cloze Test to replace the state-of-the-art for evaluating narrative structure learning, the 'Narrative Cloze Test'.
To enable the Story Cloze Test, we created a new corpus of five-sentence commonsense stories, 'ROCStories'. This corpus is unique in two ways: (1) it captures a rich set of causal and temporal commonsense relations between daily events, and (2) it is a high quality collection of everyday life stories that can also be used for story generation.
## Time 
2016
## Tag
Recognizing Textual Entailment(RTE), Reading Comprehension
## Organization
(1) University of Rochester
(2) United States Naval Academy
(3) Microsoft Research
(4) Virginia Tech
(5) The Institute for Human & Machine Cognition
## Language 
English
## Target

# THCHS-30 Dataset
## Source 
[THCHS-30 : A Free Chinese Speech Corpus](https://arxiv.org/pdf/1512.01882v2.pdf)
## Link
[Dataset Link](http://data.cslt.org/thchs30/README.html)
## Description
THCHS30 is an open Chinese speech database published by Center for Speech and Language Technology (CSLT) at Tsinghua University.
The originonal recording was conducted in 2002 by Dong Wang, supervised by Prof. Xiaoyan Zhu, at the Key State Lab of Intelligence and System,
Department of Computer Science, Tsinghua Universeity, and the original name was 'TCMSD', standing for 'Tsinghua Continuous Mandarin Speech Database'. The publication after 13 years has been initiated by Dr. Dong Wang and was supported by Prof. Xiaoyan Zhu. We hope to provide a toy database for new researchers in the field of speech recognition. Therefore,the database is totally free to academic users. 
The entire package involves the full set of speech and language resources required to establish a Chinese  speech recognition system.
## Time 
2015
## Tag
Speech Recognition
## Organization
(1) Center for Speech and Language Technology, Research Institute of
Information Technology, Tsinghua University
## Language
Chinese
## Target

# SQUINKY Dataset
## Source
[SQUINKY! A Corpus of Sentence-level Formality,Informativeness, and Implicature](https://arxiv.org/pdf/1506.02306v1.pdf)
## Link
[Dataset Link](https://drive.google.com/file/d/0B2Mzhc7popBgdXZmRlg2RUdqdDA/view)
## Description
We introduced a dataset of 7,032 sentences rated for formality, informativeness, and implicature on a 1-7 scale by human annotators on Amazon Mechanical Turk. To the best of our knowledge, this is the first large-scale annotation effort that ties together all three pragmatic variables at the sentence level.
## Time
2015
## Tag
Formality, Informativeness, Implicature
## Organization
(1)Computer Science and Engineering, University of Michigan
## Language
English
## Target

# Stanford Natural Language Inference Corpus
## Source
[A large annotated corpus for learning natural language inference](https://arxiv.org/pdf/1508.05326v1.pdf)
## Link
[Dataset Link](http://nlp.stanford.edu/projects/snli/)
## Description
The SNLI corpus (version 1.0) is a collection of 570k human-written English sentence pairs manually labeled for balanced classification with the labels entailment, contradiction, and neutral, supporting the task of natural language inference (NLI), also known as recognizing textual entailment (RTE). We aim for it to serve both as a benchmark for evaluating representational systems for text, especially including those induced by representation learning methods, as well as a resource for developing NLP models of any kind.
## Time
2015
## Tag
Natural Language Inference
## Organization
(1) Stanford Linguistics 
(2) Stanford NLP Group 
(3) Stanford Computer Science
## Language
English
## Target

# MOSI Dataset
## Source
[MOSI: Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis in Online Opinion Videos](http://cn.arxiv.org/ftp/arxiv/papers/1606/1606.06259.pdf)
## Link
Please contact abagherz@cs.cmu.edu or morency@cs.cmu.edu for accessing the dataset.
## Description
We introduce the Multimodal Opinion-level Sentiment Intensity (MOSI) dataset
which contains: (1) multimodal observations including transcribed speech and visual gestures as well as automatic audio and visual features, (2)
opinion-level subjectivity segmentation, (3) sentiment intensity annotations with high coder agreement, and (4) alignment between words, visual and acoustic features. The following sub-sections are describing the dataset in more details.
## Time
2016
## Tag
Multi-Modal, Video, Sentiment
## Organization
(1) Carnegie Mellon University, Pittsburgh
(2) University of Washington, Seattle
(3) University of Southern California, Los Angeles, CA 
## Language
English
## Target

















 

