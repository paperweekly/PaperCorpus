# Ubuntu Dialogue Corpus
## Source
[The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems](http://aclweb.org/anthology/W/W15/W15-4640.pdf)
## Link
[Dataset Link](https://github.com/rkadlec/ubuntu-ranking-dataset-creator)
## Description
a dataset containing almost 1 million multi-turn dialogues, with a total of over 7 million utterances and 100 million words. This provides a unique resource for research into building dialogue managers based on neural language models that can make use of large amounts of unlabeled data. 
## Time
2015
## Tag
Multi-Turn Dialogue System
## Organization
(1)School of Computer Science, McGill University, Montreal, Canada
(2)Dept Computer Science and Operations Research, Université de Montréal, Montreal, Canada
## Language
English
## Target
(1)[Improved Deep Learning Baselines for Ubuntu Corpus Dialogs](http://arxiv.org/pdf/1510.03753v2.pdf)

# LAMBADA Dataset
## Source
[The LAMBADA dataset:Word prediction requiring a broad discourse context](http://aclweb.org/anthology/P/P16/P16-1144.pdf)
## Link 
[Dataset Link](http://clic.cimec.unitn.it/lambada/)
## Description
A dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word. To succeed on LAMBADA,computational models cannot simply rely on local context, but must be able to keep track of information in the broader discourse.
## Time
2016
## Tag
Word Prediction Task
## Language
English
## Organization
(1)CIMeC - Center for Mind/Brain Sciences, University of Trento
(2)Institute for Logic, Language & Computation, University of Amsterdam
## Target

# CNN/Daily Mail Dataset
## Source
[Teaching Machines to Read and Comprehend](http://cn.arxiv.org/pdf/1506.03340.pdf)
## Link
[Dataset Link](https://github.com/deepmind/rc-data)
## Description
Inspired by work in summarization, we create two machine reading corpora by exploiting online newspaper articles and their matching summaries. We have collected 93k articles from the CNN and 220k articles from the Daily Mail websites. Both news providers supplement their articles with a number of bullet points, summarizing aspects of the information contained in the article. Of key importance is that these summary points are abstractive and do not simply copy sentences from the documents. We construct a corpus of document–query–answer triples by turning these bullet points into Cloze style questions by replacing one entity at a time with a placeholder.
## Time
2015
## Tag
Machine Reading Comprehension
## Organization
(1)Google DeepMind 
(2)University of Oxford
## Language
English
## Target

#  Children’s Book Test Dataset
## Source
[THE GOLDILOCKS PRINCIPLE: READING CHILDREN’S BOOKS WITH EXPLICIT MEMORY REPRESENTATIONS](http://cn.arxiv.org/pdf/1511.02301.pdf)
## Link
[Dataset Link](http://www.thespermwhale.com/jaseweston/babi/CBTest.tgz)
## Description
The CBT is built from books that are freely available thanks to Project Gutenberg. Using children’s books guarantees a clear narrative structure, which can make the role of context more salient.
In each question, the first 20 sentences form the context (denoted S), and a word (denoted a) is removed from the 21st sentence, which becomes the query (denoted q). Models must identify the answer word a among a selection of 10 candidate answers (denoted C) appearing in the context sentences and the query.
## Time
2016
## Tag
Machine Reading Comprehension
## Organization
Facebook AI Research
## Language
English
## Target


#  SQuAD Dataset
## Source
[SQuAD: 100,000+ Questions for Machine Comprehension of Text](http://cn.arxiv.org/pdf/1606.05250v1)
## Link
[Dataset Link](https://rajpurkar.github.io/SQuAD-explorer/)
## Description
SQuAD, consisting of 100,000+ questions posed by crowdworkers on a set of
Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage.
## Time
2016
## Tag
Machine Reading Comprehension 
## Organization
Computer Science Department, Stanford University
## Language
English
## Target
(1) [MACHINE COMPREHENSION USING MATCH-LSTM AND ANSWER POINTER](http://cn.arxiv.org/pdf/1608.07905v1.pdf)

# People Daily and Children’s Fairy Tale Datasets
## Source
[Consensus Attention-based Neural Networks for Chinese Reading Comprehension](https://arxiv.org/pdf/1607.02250v2.pdf)
## Link
[Dataset Link](http://hfl.iflytek.com/chinese-rc/)
## Description
Though many solid works on previously described public datasets, there is no studies on Chinese reading comprehension datasets.
(1) As far as we know, this is the first Chinese Cloze-style reading comprehension datasets.
(2) We provide a large-scale Chinese reading comprehension data in news domain, as well as its validation and test data as the in-domain test.
(3) Further, we release two out-of-domain test sets, and it deserves to highlight that one of the test sets is made by the humans, which makes it harder to answer than the automatically generated test set.
## Time
2016
## Tag
Machine Reading Comprehension 
## Organization
(1) iFLYTEK Research, China
(2) Research Center for Social Computing and Information Retrieval,
Harbin Institute of Technology, China
## Language 
Chinese
## Target
(1) [Attention-over-Attention Neural Networks for Reading Comprehension](https://arxiv.org/pdf/1607.04423v3.pdf)

# Factoid Question-Answer Corpus
## Source
[Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus](https://arxiv.org/pdf/1603.06807v2.pdf)
## Link
[Dataset Link](http://agarciaduran.org/)
## Description
We present the 30M Factoid QuestionAnswer Corpus, an enormous question answer pair corpus produced by applying a novel neural network architecture on the knowledge base Freebase to transduce facts into natural language questions. The produced question answer pairs are evaluated both by human evaluators and using automatic evaluation metrics, including well-established machine translation and sentence similarity metrics.
## Time
2016
## Tag
Question Answer 
## Organization
(1) University of Montreal, Canada
(2) Universite de Technologie de Compiegne - CNRS, France
## Language 
English
## Target

# LCSTS Dataset
## Source
[LCSTS: A Large-Scale Chinese Short Text Summarization Dataset](http://arxiv.org/pdf/1506.05865v4.pdf)
## Link
[Dataset Link](http://icrc.hitsz.edu.cn/Article/show/139.html)
## Description
We introduce a Large-scale Chinese Short Text Summarization dataset constructed from the Chinese microblogging website SinaWeibo. This corpus consists of over 2 million real Chinese short texts with short summaries given by the writer of each text. We also manually tagged the relevance of 10,666 short summaries with their corresponding short texts. 
## Time
2015
## Tag
Text Summarization
## Organization
(1) Intelligent Computing Research Center, Harbin Institute of Technology, Shenzhen Graduate School
## Language 
Chinese
## Target














 

